{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5563d0c8",
   "metadata": {
    "rise": {
     "backimage": "image2.jpg"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"font-size:2em;padding-bottom: 0.5em; font-weight: bold;\">\n",
    "E-learning course on <br / >\n",
    "Advanced Neutron Imaging\n",
    "</p> \n",
    "\n",
    "<p style=\"font-size:1.25em;padding-bottom: 0.25em;\">\n",
    "    \n",
    "# Module Number/Introduction to images\n",
    "\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size:1em;\">February 25, 2021</p>\n",
    "<br /><br />\n",
    "<p style=\"font-size:1.25em;padding-bottom: 0.25em; text-align: center;\">Anders Kaestner</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff7a7ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-07T16:28:20.442881Z",
     "start_time": "2022-02-07T16:27:40.008320Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from skimage.io import imread\n",
    "from scipy.ndimage import convolve\n",
    "from skimage.morphology import disk\n",
    "from skimage.transform import resize\n",
    "from itertools import product\n",
    "\n",
    "import os\n",
    "from io import StringIO\n",
    "import skimage as ski"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62639516",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training objectives\n",
    "\n",
    "- Terminal Training Objectives:\n",
    "    - Obtain an awareness that numerical methods are needed for the quantitative interpretation of images.\n",
    "    - Understand the concept of an image processing workflow.\n",
    "\n",
    "- Enabling training objectives:\n",
    "    - Understanding that the analysis objectives decide the choice of strategy and tools.\n",
    "    - Basic definitions of \n",
    "        - images\n",
    "        - gray levels\n",
    "        - visibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefc5592",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What shall we do with our images?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828a384e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## You have data...\n",
    "\n",
    "Imaging experiments produce large amounts of data\n",
    "\n",
    "![Experiment data](figures/experimentdata.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6ae85a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Important questions\n",
    "\n",
    "What is the purpose of the experiment?\n",
    "\n",
    "- Qualitative visual analysis using 3D visualization\n",
    "- Sample characterization\n",
    "- Determine process parameters\n",
    "- Count and measure size of features\n",
    "- etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1367e644",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Which information do you expect from the data\n",
    "\n",
    "\n",
    "| Intensity  | Geometry |\n",
    "|:----------|:----------|\n",
    "| Material composition | Identify features |\n",
    "| Material transport   | Volume |\n",
    "| Physical quantities | Shape |\n",
    "\n",
    "The information you want to quantify affects:\n",
    "- The choice of processing method\n",
    "- The experiment strategy \n",
    "- The choice of analysis tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecd95d0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Measurements are rarely perfect\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/imperfect_imaging_system.svg\" style=\"height:350px\">\n",
    "</center>\n",
    "\n",
    "Relevant features versus:\n",
    "- Resolution\n",
    "- Sample movement\n",
    "- Noise\n",
    "- Inhomogeneous contrast\n",
    "- Artefacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628f308f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Questions\n",
    "1. Why \n",
    "2. How\n",
    "3. When"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420810c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f92bec",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Define an image\n",
    "\n",
    "### What is an image?\n",
    "\n",
    "A very abstract definition: \n",
    "- __A pairing between spatial information (position)__\n",
    "- __and some other kind of information (value).__\n",
    "\n",
    "In most cases this is a 2- or 3-dimensional position (x,y,z coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0645bd98",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Image sampling\n",
    "| The world is | The computer needs|\n",
    "|:---:|:---:|\n",
    "| Continuous    | Discrete levels |\n",
    "| No boundaries | Limited extent | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccc1581",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "A real world observation is contious and we need to sample the information in order to bring it into the computer for storage and to perform our analysis. There are different types of sampling involved to get a digital image. \n",
    "\n",
    "Intensity \n",
    ": The continous intensity must be sampled into discrete levels represented by a number of bits\n",
    "\n",
    "Locations\n",
    ":The location in the scene must be rasterized into a grid of pixels that contain the image intensity. The size of the pixels determine the greatest spatial frequency that can be respresented in the image. The spatial sampling follows the Nyquist sampling theorem that says that you have to sample by twice as fast the highest frequency in the scene. If you sample too slow you will see aliasing effects that appear as moiree fringes in the image.   \n",
    "\n",
    "```{figure} figures/grid.pdf\n",
    "---\n",
    "scale: 75%\n",
    "---\n",
    "The real world is sampled into discrete images with limited extent.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9386a7b6",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"figures/grid.svg\" style=\"height:400px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8269de3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Different types of images\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>2D</center></th>\n",
    "        <th><center>3D</center></th>\n",
    "        <th><center>4D</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"figures/plane_10x10.svg\" style=\"height:50px\"></td>\n",
    "        <td><img src=\"figures/cube_10x10x10.svg\" style=\"height:100px\">\n",
    "            <img src=\"figures/timeseries_visualization.svg\" style=\"height:100px\">\n",
    "        </td>\n",
    "        <td><img src=\"figures/4D-images.svg\" style=\"height:100px\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Radiographs</td>\n",
    "        <td>Tomography<br/>Time-series<br/>Spectrum</td>\n",
    "        <td>Volume time series</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d1ed41",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pixel size and resolution \n",
    "\n",
    "It is important to distinguish between pixel size and resolution\n",
    "\n",
    "### Pixel size\n",
    "The pixel size is\n",
    "- The sample pitch between two adjacent pixels\n",
    "- The smallest area represented in the image\n",
    "\n",
    "### Resolution \n",
    "The resolution is related to the optical system\n",
    "- It is the effect of the optical transfer function of the acquisition system.\n",
    "- Should have a greater value than the pixel size.\n",
    "- Defines the smallest pixel size when you set up your acquistion conditions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9db203",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Demonstrating different pixel sizes\n",
    "What happens when we represent the same image with less pixels?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f202aec",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In this example we downsample the image first by a factor two. This change is barely visible when we show the image, but the number of pixels have reduced by a factor four. In the second example the image is downscaled by a factor 32 and you can clearly observe how pixelated the image is. A this level of downscaling, you can only see very coarse features in the sample.\n",
    "\n",
    "Down scaling is sometimes used as a method to speed up the frame rate as it radiacally reduces the number of bytes to be transfered from the detector and also the amount of data to write on disk. You should however be careful not to down scale by a too great factor as you will loose spatial information when doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a09f402",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-07T16:28:30.347273Z",
     "start_time": "2022-02-07T16:28:29.415454Z"
    }
   },
   "outputs": [],
   "source": [
    "img=np.load('data/wood.npy');\n",
    "fig,ax = plt.subplots(1,3, figsize=[15,5])\n",
    "ax[0].imshow(img,cmap='gray'); plt.title('Original')\n",
    "\n",
    "downsize =  2; \n",
    "resized  = resize(img,(img.shape[0] // downsize, img.shape[1] // downsize))\n",
    "ax[1].imshow(resized, interpolation='None',cmap='gray'); ax[1].set_title('Downsize {0}x{0}'.format(downsize))\n",
    "\n",
    "downsize = 32; \n",
    "resized  = resize(img,(img.shape[0] // downsize, img.shape[1] // downsize))\n",
    "ax[2].imshow(resized,interpolation='None',cmap='gray'); ax[2].set_title('Downsize {0}x{0}'.format(downsize));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea35c21c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Edges at different resolutions and pixel sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49c30a0",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Finding the correct pixel size is related to the resolution of the imaging system. You can sample low resolved scenes with many pixels but then the edges will appear blurred and will essentially waste a lot data on little added value.\n",
    "\n",
    "The example below shows what an ideal edge would look like and what it mostly looks like when we acquire our images. As you can see, the \"real\" edge is represented by a smooth transition spread over several pixels.\n",
    "\n",
    "```{figure} figures/edge.pdf\n",
    "---\n",
    "scale: 75%\n",
    "---\n",
    "Examples of edges sampled with different pixel sizes.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078817ea",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"figures/edges.svg\" style=\"height:600px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ceb732",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Image intensity\n",
    "What happens when we reduce the number of gray-levels in the image?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64143a41",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The image intensity is determined by the response function of the imaging system. In the case of neutron imaging we are talking about the transmission of the neutron beam through the sample. The transmission follows Beer-Lambert's law\n",
    "$I(x,y)=I(x,y) e^{-\\int_L \\mu(x) dx}$, this is only a simplified version. More complicated versions including the neutron energy are presented in other parts of this course.\n",
    "\n",
    "The information captured by the detector is stored in digital form with different gray level dynamics. We are often talking about 8 or 16 bit integer when we store images. This means that each pixel can represent the measured intensity with either 256 or 65565 gray levels respectively. In the example below we demonstrate what happens when only very few gray levels are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d19f65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-07T16:28:33.129468Z",
     "start_time": "2022-02-07T16:28:32.189603Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img=np.load('data/wood.npy');\n",
    "fig,ax = plt.subplots(1,3,figsize=[15,7])\n",
    "ax[0].imshow(img, cmap='gray'); plt.title('Original')\n",
    "\n",
    "levels   = 16; \n",
    "lvl = np.floor(img*levels)\n",
    "ax[1].imshow(lvl, cmap='gray'); ax[1].set_title('{0} Levels'.format(levels));\n",
    "\n",
    "levels   = 4 ; \n",
    "lvl = np.floor(img*levels)\n",
    "ax[2].imshow(lvl, cmap='gray'); ax[2].set_title('{0} Levels'.format(levels));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88becfcd",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "It is important to use as many gray levels at possible when you expose your images. The image turns patchy when you use too few levels which you can see in the example above. The patchiness reduces the precision of your evaluation, there is less margin to make estimations and decisions. The number of gray levels depend on many factors like:\n",
    "\n",
    "- Exposure time\n",
    "- Neutron flux\n",
    "- Conversion efficiency\n",
    "- Conversion rate of the detector\n",
    "- Pixel size\n",
    "\n",
    "So, it is your task to optimize your acquisition to provide well illuminated images by changing these parameters. Some are easier than others to change and contstraint are given by the type of investigation you are doing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494319a2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How many bits are needed?\n",
    "\n",
    "The number of bits you need depends on:\n",
    "- Contrast difference\n",
    "- Separate many different sample features\n",
    "- Sensitivity to rounding errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6339c1",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The table below gives you an idea how many bits you need to represent your image information. In the extreme you would only need a single bit per pixel (8 pixels per byte) to represent a bi-level image from a segmentation. The other extreme would be to use double precision floating point that requires 64 bits (8 bytes) per pixels. Double precision is rarely needed and single precision is mostly sufficient which saves you memory. Saving memory is in particular important when you work with 3D images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cbfd40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T17:21:57.808840Z",
     "start_time": "2022-02-06T17:21:57.801994Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<span style=\"font-size:1.5em;\">\n",
    "\n",
    "| Few bits | Many bits | Floating point |\n",
    "|:----------|:-----------|:----------------|\n",
    "|High contrast<br/>Clean images<br/>Segmented data|Low contrast<br/>Noisy images<br/>Gradual changes|High intensity dynamics<br/>Quantification to physical properties<br/>In algorithms|\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8531f1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The histogram\n",
    "\n",
    "The histogram is a statistical tool to show frequency of each graylevel in the image. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ae86fb",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "It is essentially a plot where the you count how many times each gray level appears in the image. For a 16-bit image this would result 65565 points. This is far too detailed therefore it is common to use bins of several gray level to reduce the level of detail in the histogram and also improve the readability. In the example below we use 100 histogram bins which look quite reasonable for this image. Chosing the number of bins depends on the image size too. Your histogram doesn't look very useful if you have too many bins compared to the available number of pixels. You can use the piece of code below to explore what happens when you change the number of bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aadddb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-07T16:28:36.050721Z",
     "start_time": "2022-02-07T16:28:35.098367Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "# Compute and show a histogram\n",
    "ax[0].hist(img.ravel(),bins=200)\n",
    "\n",
    "\n",
    "ax[0].set_xlabel('Image value'), ax[0].set_ylabel('Number of pixels')\n",
    "ax[1].imshow(img,cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382a4c30",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "It can be used for the analysis of the image as it gives you an idea which values are related to different features in the image. The histogram tells you the area covered by a give pixel value and a later section we will see how the histogram can be used to segment the images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49d871b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Details of the histogram\n",
    "Let's look att different regions in the image and their representation in the histogram.\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/histogram_regions.svg\" style=\"height:400px\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde42215",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T07:29:21.768081Z",
     "start_time": "2022-02-04T07:29:21.762397Z"
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```{figure} figures/histogram_regions.pdf\n",
    "---\n",
    "scale: 75%\n",
    "---\n",
    "Regions in the image connected to their position in the histogram.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b835485a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Histogram examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8401b33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-07T16:28:39.988909Z",
     "start_time": "2022-02-07T16:28:37.293310Z"
    },
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "a=plt.imread('figures/testpattern_noisy.jpg')\n",
    "b=plt.imread('figures/neutron_camera.png')\n",
    "c0=plt.imread('figures/root_slices.png')\n",
    "c1=plt.imread('figures/root_histogram.png')\n",
    "\n",
    "fig,ax=plt.subplots(2,3,figsize=(15,8))\n",
    "ax=ax.ravel()\n",
    "\n",
    "ax[0].imshow(a,cmap='gray')\n",
    "ax[0].set_xticks([]);ax[0].set_yticks([])\n",
    "ax[0].set_title('Noisy bi-level image')\n",
    "ax[3].hist(a.ravel(),bins=100)\n",
    "ax[1].imshow(b,cmap='gray')\n",
    "ax[1].set_title('Neutron image')\n",
    "ax[1].set_xticks([]);ax[1].set_yticks([])\n",
    "ax[4].hist(b.ravel(),bins=100)\n",
    "ax[2].imshow(c0)\n",
    "ax[2].set_title('Bivariate data')\n",
    "ax[2].axis('off')\n",
    "ax[5].imshow(c1)\n",
    "ax[5].axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c77661",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Visualizing gray levels\n",
    "\n",
    "The human eye is not able to resolve many intensity levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ff878b",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In this example we demonstrate how easy or hard it can be to perceive contrast differences in an image with different number of gray levels. How well you can see the differences depends on one hand on how well your eye can resolve the contrast difference, but there are also technical issues related to how well you can see the changes. E.g. how well your screen can display the changes and even the ambient light in the room you are working in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce1a5ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-07T16:28:40.726309Z",
     "start_time": "2022-02-07T16:28:40.010521Z"
    },
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "xlin = np.linspace(0,255, 256)\n",
    "xx, yy = np.meshgrid(xlin, xlin)\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize = (15, 5))\n",
    "n=32; ax[0].imshow(np.floor(xx/n), interpolation='None', cmap = 'gray');  ax[0].set_title('{0} levels'.format(256//n))\n",
    "n=8;  ax[1].imshow(np.floor(xx/n), interpolation='None', cmap = 'gray');  ax[1].set_title('{0} levels'.format(256//n))\n",
    "n=1;  ax[2].imshow(np.floor(xx/n), interpolation='None', cmap = 'gray');  ax[2].set_title('{0} levels'.format(256//n));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7bb26b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Using pseudo colormaps\n",
    "\n",
    "The image intensity is mostly only represented by a scalar by a gray level. Which is makes it hard to see subtle changes in intensity. Colormaps can help here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa85800f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-07T16:28:41.867507Z",
     "start_time": "2022-02-07T16:28:40.729897Z"
    }
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,3,figsize=(15,5))\n",
    "ax[0].imshow(img,cmap='gray');    ax[0].set_title('Gray')\n",
    "ax[1].imshow(img,cmap='viridis'); ax[1].set_title('Viridis')\n",
    "ax[2].imshow(img,cmap='hot');     ax[2].set_title('Hot');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0947e5c",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Colormaps are only used for the visulization making it possible to better visualize and highlight features in the image. They can however also be misleading if you chose the wrong colormap. The interpretation of the image is in particular hard when you start manipulating the colormap. In this way it is even posible to \"invent\" features in the image that are not real. A typical example is that you thanks to the colormap could see a denser skin like structure near the sample boundary. It the reality this \"skin\" is only the smooth edge which is caused by low resolution of the imaging system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba555ca",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Brightness\n",
    "With image brightness, you can focus on narrow gray level intervals to better visually resolve local details.\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/histogram_brightness.svg\" style=\"height:500px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e7f7ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T19:03:40.005724Z",
     "start_time": "2022-02-04T19:03:39.993585Z"
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```{figure} figures/histogram_brightness.pdf\n",
    "---\n",
    "scale: 75%\n",
    "---\n",
    "Narrow intensity intervals to highlight low (left) and high (right) graylevel regions.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0db50b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T18:29:28.371180Z",
     "start_time": "2022-02-04T18:29:28.368553Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Contrast\n",
    "\n",
    "Contrast controls the width of the intensity interval to use.\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/histogram_contrast.svg\" style=\"height:500px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f792776",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Contrast control is often used to define which gray levels to include when you save image to file. The image is usually represented in floating point data format after some calculations and you have to limit the interval to resolve the relevant information with many gray levels and reject outliers when you convert to 8- or 16-bit integers. \n",
    "\n",
    "The example shows a narrow interval that mostly is useful to highlight features with small difference in contrast. The example with wider interval would is set to reject the background while most of the sample is visible. This setting may be useful for presentationations and publication where you want to boost the visibility, but is not recommended if you want to use the image in further calculations. In the latter case it is important to keep as many gray levels as possible, i.e. also include the noise flucuations in the background.\n",
    "\n",
    "```{figure} figures/histogram_contrast.pdf\n",
    "---\n",
    "scale: 75%\n",
    "---\n",
    "Different image contrasts. Narrow interval to the left and wide interval to the right.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab9f4d3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pixelwise operations\n",
    "\n",
    "Pixelwise operations apply scalar operations to each pixel.\n",
    "- Arithmetics +, -, *, /\n",
    "- Functions e.g. sin(x), exp(x), ln(x)\n",
    "\n",
    "Statistic functions\n",
    "- mean, standard deviation\n",
    "- min, max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d759279e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-05T17:24:41.128891Z",
     "start_time": "2022-02-05T17:24:41.120582Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Demonstrating arithmetic functions (flat field normalization)\n",
    "\n",
    "$$normed = \\frac{img-dc}{ob-dc}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663ad581",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-07T16:28:43.095955Z",
     "start_time": "2022-02-07T16:28:41.881010Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = plt.imread('data/wood_0000.tif')\n",
    "ob  = plt.imread('data/ob_0000.tif')\n",
    "dc  = plt.imread('data/dc_0000.tif')\n",
    "\n",
    "normed = (img-dc)/(ob-dc)\n",
    "\n",
    "fig,ax = plt.subplots(1,4,figsize=(15,4))\n",
    "ax[0].imshow(img,    cmap='gray'); ax[0].set_title('Sample image')\n",
    "ax[1].imshow(ob,     cmap='gray'); ax[1].set_title('Open beam image')\n",
    "ax[2].imshow(dc,     cmap='gray'); ax[2].set_title('Dark current image')\n",
    "ax[3].imshow(normed, cmap='gray'); ax[3].set_title('Normalized image');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22926f81",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Normalization with loops\n",
    "Pixel wise operations can also be implemented using loops. This results in more complicated code and is mostly less efficent in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d04a43b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-07T16:28:44.647901Z",
     "start_time": "2022-02-07T16:28:43.100436Z"
    }
   },
   "outputs": [],
   "source": [
    "normed = np.zeros(img.shape)\n",
    "\n",
    "for r in range(img.shape[0]):\n",
    "    for c in range(img.shape[1]):\n",
    "        normed[r,c]=(img[r,c]-dc[r,c])/(ob[r,c]-dc[r,c])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cb3910",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Questions\n",
    "1. How many gray levels can the human eye percieve? Try to find your limit using the code.\n",
    "2. What does the histogram tell you about an image?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec48de84",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary \n",
    "\n",
    "In this lecture you learned about:\n",
    "1. Images and pixels.\n",
    "2. Gray level dynamics and how to visualize them.\n",
    "2. The histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded45797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "rise": {
   "autolaunch": true,
   "backimage": "image1.png",
   "enable_chalkboard": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
